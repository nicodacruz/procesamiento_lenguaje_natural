{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LlqmtEW1Hn"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "import bs4 as bs\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "tFALNLwTIgzu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus"
      ],
      "metadata": {
        "id": "aButT2JPIgJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.textos.info/miguel-de-cervantes-saavedra/el-ingenioso-hidalgo-don-quijote-de-la-mancha/ebook\"\n",
        "\n",
        "raw_html = urllib.request.urlopen(url).read()\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, \"lxml\")\n",
        "\n",
        "article_paragraphs = article_html.find_all(\"p\")\n",
        "\n",
        "article_text = \" \".join(p.text for p in article_paragraphs)\n",
        "\n",
        "article_text = article_text.lower()\n",
        "\n",
        "print(\"Cantidad de caracteres en el corpus (antes de recorte):\", len(article_text))\n",
        "print(article_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwCYmTX4IpIp",
        "outputId": "892201db-2f21-4f33-8200-7eef02786683"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de caracteres en el corpus (antes de recorte): 2077830\n",
            " yo, juan gallo de andrada, escribano de cámara del rey nuestro señor, de\n",
            "los que residen en su consejo, certifico y doy fe que, habiendo visto por\n",
            "los señores dél un libro intitulado el ingenioso hidalgo de la mancha,\n",
            "compuesto por miguel de cervantes saavedra, tasaron cada pliego del dicho\n",
            "libro a tres maravedís y medio; el cual tiene ochenta y tres pliegos, que\n",
            "al dicho precio monta el dicho libro docientos y noventa maravedís y medio,\n",
            "en que se ha de vender en papel; y dieron licencia para que a este precio\n",
            "se pueda vender, y mandaron que esta tasa se ponga al principio del dicho\n",
            "libro, y no se pueda vender sin ella. y, para que dello conste, di la\n",
            "presente en valladolid, a veinte días del mes de deciembre de mil y\n",
            "seiscientos y cuatro años. juan gallo de andrada. este libro no tiene cosa digna que no corresponda a su original; en\n",
            "testimonio de lo haber correcto, di esta fee. en el colegio de la madre de\n",
            "dios de los teólogos de la universidad de alcalá, en primero de diciembre\n",
            "de 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilicé como corpus *“Don Quijote de la Mancha”* en formato ebook desde textos.info. Después de descargar y parsear el HTML, uní todos los párrafos en un único string y convertí el texto a minúsculas. El resultado es un texto largo en español, suficiente para que el modelo aprenda patrones básicos de ortografía, puntuación y estilo."
      ],
      "metadata": {
        "id": "StVQfX1EW-OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenización de caracteres con Keras"
      ],
      "metadata": {
        "id": "C3bNRS5BIqRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizador a nivel carácter\n",
        "tokenizer = Tokenizer(\n",
        "    char_level=True,\n",
        "    filters=\"\",\n",
        "    lower=False,\n",
        "    oov_token=None\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts([article_text])\n",
        "\n",
        "char2idx = tokenizer.word_index.copy()\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "\n",
        "vocab_size = len(char2idx) + 1\n",
        "\n",
        "print(\"Tamaño vocabulario (nº de caracteres diferentes):\", vocab_size)\n",
        "print(\"Algunos pares char -> idx:\", list(char2idx.items())[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yCF5bfwIsDv",
        "outputId": "8df079db-8ee2-48c8-cd20-1d6f4d620a8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño vocabulario (nº de caracteres diferentes): 66\n",
            "Algunos pares char -> idx: [(' ', 1), ('e', 2), ('a', 3), ('o', 4), ('s', 5), ('n', 6), ('r', 7), ('l', 8), ('d', 9), ('u', 10), ('i', 11), ('t', 12), ('c', 13), ('m', 14), (',', 15), ('p', 16), ('q', 17), ('\\n', 18), ('y', 19), ('b', 20), ('h', 21), ('v', 22), ('g', 23), ('í', 24), ('j', 25), ('ó', 26), ('.', 27), ('f', 28), ('é', 29), ('á', 30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texto codificado y construcción del dataset"
      ],
      "metadata": {
        "id": "dmhPmgVQIveQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences([article_text])[0]\n",
        "print(\"Largo de la secuencia codificada:\", len(encoded))\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "sequences = []\n",
        "\n",
        "for i in range(seq_length, len(encoded)):\n",
        "    seq = encoded[i - seq_length : i + 1]\n",
        "    sequences.append(seq)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "print(\"Shape de sequences (total_secuencias, seq_length+1):\", sequences.shape)\n",
        "\n",
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "\n",
        "print(\"Shape X:\", X.shape)\n",
        "print(\"Shape y:\", y.shape)\n",
        "\n",
        "split_idx = int(len(X) * 0.9)\n",
        "\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "X_train = X_train.astype(\"int32\")\n",
        "X_val   = X_val.astype(\"int32\")\n",
        "y_train = y_train.astype(\"int32\")\n",
        "y_val   = y_val.astype(\"int32\")\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \" | y_train:\", y_train.shape)\n",
        "print(\"X_val:  \", X_val.shape,   \" | y_val:  \", y_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M5YCdN-IuCw",
        "outputId": "9617b6bb-573a-4496-82cd-8724a115d505"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largo de la secuencia codificada: 2077830\n",
            "Shape de sequences (total_secuencias, seq_length+1): (2077730, 101)\n",
            "Shape X: (2077730, 100)\n",
            "Shape y: (2077730,)\n",
            "X_train: (1869957, 100)  | y_train: (1869957,)\n",
            "X_val:   (207773, 100)  | y_val:   (207773,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apliqué una tokenización a nivel carácter usando `Tokenizer` de Keras, asignando un índice entero a cada símbolo distinto del texto. Con la secuencia codificada construí ventanas de longitud fija (`seq_length = 100`), donde cada muestra usa 100 caracteres como entrada y el carácter siguiente como etiqueta. Finalmente separé el conjunto en entrenamiento y validación (90% / 10%) para evaluar la capacidad de generalización del modelo."
      ],
      "metadata": {
        "id": "39NM4hRhWHew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo Keras (Embedding + LSTM)"
      ],
      "metadata": {
        "id": "EcIadSb8LjM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "rnn_units = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=seq_length\n",
        "    ),\n",
        "    LSTM(rnn_units, return_sequences=False),\n",
        "    Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=Adam(learning_rate=0.001)\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "VyuytU3fIwRU",
        "outputId": "26177685-7ef5-49fe-ca47-ca407d325123"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "Sh3S65hELn6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5j6SbxhLmqV",
        "outputId": "da88b31f-0361-4064-be2f-7a626870830c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 9ms/step - loss: 2.0024 - val_loss: 1.5932\n",
            "Epoch 2/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 9ms/step - loss: 1.5442 - val_loss: 1.5002\n",
            "Epoch 3/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 9ms/step - loss: 1.4591 - val_loss: 1.4570\n",
            "Epoch 4/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 9ms/step - loss: 1.4184 - val_loss: 1.4319\n",
            "Epoch 5/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 9ms/step - loss: 1.3941 - val_loss: 1.4158\n",
            "Epoch 6/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 9ms/step - loss: 1.3763 - val_loss: 1.4056\n",
            "Epoch 7/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 9ms/step - loss: 1.3632 - val_loss: 1.4006\n",
            "Epoch 8/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 9ms/step - loss: 1.3548 - val_loss: 1.3932\n",
            "Epoch 9/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 10ms/step - loss: 1.3465 - val_loss: 1.3870\n",
            "Epoch 10/10\n",
            "\u001b[1m14610/14610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 9ms/step - loss: 1.3370 - val_loss: 1.3824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "El modelo está formado por una capa `Embedding`, una capa `LSTM` con 128 unidades y una capa densa final con activación `softmax` sobre todo el vocabulario de caracteres. Lo entrené con `sparse_categorical_crossentropy` y el optimizador `Adam`, usando batches intermedios y varias épocas. Durante el entrenamiento la pérdida de entrenamiento y validación disminuye de forma estable, lo que muestra que el modelo va capturando patrones del corpus sin evidenciar un sobreajuste fuerte."
      ],
      "metadata": {
        "id": "RcSdUY0aXLCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función para generar texto"
      ],
      "metadata": {
        "id": "3f_TtmKFLry2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds + 1e-8) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(preds), p=preds)\n",
        "\n",
        "def generate_text(seed_text, gen_length=500, temperature=1.0):\n",
        "    text = seed_text.lower()\n",
        "\n",
        "    for _ in range(gen_length):\n",
        "        encoded_seed = tokenizer.texts_to_sequences([text[-seq_length:]])[0]\n",
        "\n",
        "        if len(encoded_seed) < seq_length:\n",
        "            encoded_seed = [0] * (seq_length - len(encoded_seed)) + encoded_seed\n",
        "        else:\n",
        "            encoded_seed = encoded_seed[-seq_length:]\n",
        "\n",
        "        x_pred = np.array(encoded_seed)[None, :]\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample_with_temperature(preds, temperature=temperature)\n",
        "        next_char = idx2char.get(next_index, \"\")\n",
        "\n",
        "        text += next_char\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "NdFBrJPWLp50"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test de generación de texto"
      ],
      "metadata": {
        "id": "1iTF3xtjLuMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"en un lugar de la mancha\"\n",
        "generated = generate_text(seed_text=seed, gen_length=500, temperature=0.8)\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrUtKgbFLtqS",
        "outputId": "9b74b829-eee9-438d-d87b-a05d240fdecc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en un lugar de la mancha, si\n",
            "no se diese sin hacer, estos\n",
            "remosas que respondió el aina y ha talla que llegan satisfación en la hija; los cuales de aquella mayor corresponete. pero no hay todos: esto había\n",
            "oído mandatarle la legua que con pedras; porque corre que llevase ellos no fueran volver en los digos, en priesco del mundo el todo la boca que se pienso, las hallado mujer a mismo ella caballero, hecho ser demalo con las\n",
            "mejores cañías de los aconsejos, y le\n",
            "ha de todos aquellos a la tierra. y todo estará\n",
            "así lo que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar el modelo generé texto a partir de una frase semilla, por ejemplo “en un lugar de la mancha”. El modelo produce secuencias en español con rasgos del estilo del corpus, aunque todavía aparecen palabras inventadas y la coherencia se pierde en textos más largos. La temperatura de muestreo permite ajustar el equilibrio entre diversidad y estabilidad, valores bajos en la temperatura generan texto más predecible y repetitivo, mientras que valores altos producen resultados más creativos pero también más ruidosos."
      ],
      "metadata": {
        "id": "dB_tW46BXPs1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}